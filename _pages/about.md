---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I'm a master student at Huazhong University of Science and Technology, where I am a member of the <a href="https://hustvl.github.io/">HUST Vision Lab</a> under the supervision of <a href="https://xwcv.github.io/">Prof. Xinggang Wang</a>.
My research interests include autonomous driving, MLLMs, and generative models.

Currently, I am a research intern at Horizon Robotics, working in the team led by [Shaoyu Chen](https://scholar.google.com/citations?user=PIeNN2gAAAAJ) and [Qian Zhang](https://scholar.google.com/citations?user=pCY-bikAAAAJ). My current research focuses on Vision-Language-Action models (VLA) for autonomous driving.

# üî• News
- *2025.12*: &nbsp;üéâüéâ 

# üìù Publications 

## ‚ú® Highlights
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Visual Intelligence</div><img src='images/vitgaze.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vitgaze: gaze following with interaction features in vision transformers](https://link.springer.com/article/10.1007/s44267-024-00064-9)

**Yuehao Song**, Xinggang Wang, Jingfeng Yao, Wenyu Liu, Jinglin Zhang, Xiangmin Xu

[**PDF**](https://link.springer.com/content/pdf/10.1007/s44267-024-00064-9.pdf) \| [**Project**](https://github.com/hustvl/ViTGaze) \| [![](https://img.shields.io/github/stars/hustvl/ViTGaze
?style=social&label=Code Stars)](https://github.com/hustvl/ViTGaze) <strong><span class='show_paper_citations' data='7sqkA-MAAAAJ:d1gkVwhDpl0C'></span></strong>
</div>
</div>

## üìÑ All Publications

<div class='paper-box-light'>
<div class='paper-box-text' markdown="1">
[DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis](https://arxiv.org/abs/2512.19331)

Yueting Zhu, **Yuehao Song**, Shuai Zhang, Wenyu Liu, Xinggang Wang

[**PDF**](https://arxiv.org/pdf/2512.19331)
</div>
</div>

<div class='paper-box-light'>
<div class='paper-box-text' markdown="1">
[DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)

Jialv Zou, Shaoyu Chen, Bencheng Liao, Zhiyu Zheng, **Yuehao Song**, Lefei Zhang, Qian Zhang, Wenyu Liu, Xinggang Wang

[**PDF**](https://arxiv.org/pdf/2512.07745) \| [**Project**](https://github.com/hustvl/DiffusionDriveV2) \| [![](https://img.shields.io/github/stars/hustvl/DiffusionDriveV2
?style=social&label=Code Stars)](https://github.com/hustvl/DiffusionDriveV2)
</div>
</div>

<div class='paper-box-light'>
<div class='paper-box-text' markdown="1">

[Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)

Xiaoyan Liu, Kangrui Li, **Yuehao Song**, Jiaxin Liu

[**PDF**](https://arxiv.org/pdf/2508.07769) \| [**Project**](https://wanderer7-sk.github.io/Dream4D.github.io) \| [![](https://img.shields.io/github/stars/Wanderer7-sk/Dream4D
?style=social&label=Code Stars)](https://github.com/Wanderer7-sk/Dream4D)
</div>
</div>

<div class='paper-box-light'>
<div class='paper-box-text' markdown="1">
  
[Eva-x: A foundation model for general chest x-ray analysis with self-supervised learning](https://www.nature.com/articles/s41746-025-02032-z)

Jingfeng Yao, Xinggang Wang, **Yuehao Song**, Huangxuan Zhao, Jun Ma, Yajie Chen, Wenyu Liu, Bo Wang

**npj Digital Medicine** \| [**PDF**](https://www.nature.com/articles/s41746-025-02032-z.pdf) \| [**Project**](https://github.com/hustvl/EVA-X) \| [![](https://img.shields.io/github/stars/hustvl/EVA-X
?style=social&label=Code Stars)](https://github.com/hustvl/EVA-X)
</div>
</div>

<div class='paper-box-light'>
<div class='paper-box-text' markdown="1">

[Vitgaze: gaze following with interaction features in vision transformers](https://link.springer.com/article/10.1007/s44267-024-00064-9)

**Yuehao Song**, Xinggang Wang, Jingfeng Yao, Wenyu Liu, Jinglin Zhang, Xiangmin Xu

**Visual Intelligence** \| [**PDF**](https://link.springer.com/content/pdf/10.1007/s44267-024-00064-9.pdf) \| [**Project**](https://github.com/hustvl/ViTGaze) \| [![](https://img.shields.io/github/stars/hustvl/ViTGaze
?style=social&label=Code Stars)](https://github.com/hustvl/ViTGaze)
</div>
</div>

# üìñ Educations

- *2024.09 - Now*, Master, School of EIC, Huazhong University of Science and Technology, Wuhan, China, supervised by [Prof. Xinggang Wang](https://xwcv.github.io/).
- *2020.09 - 2024.06*, Bachelor, School of EIC, Huazhong University of Science and Technology, Wuhan, China.

# üíª Internships
- *2025.01 - Now*, [Horizon Robotics](https://cn.horizon.ai/), Beijing, China.
